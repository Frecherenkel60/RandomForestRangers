{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Rangers - Predicting Car Sales Prices\n",
    "## Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# On MacOS you need the following command\n",
    "# brew install libomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import neccessary modules\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_year        int64\n",
      "milage          float64\n",
      "accident          int64\n",
      "clean_title       int64\n",
      "price             int64\n",
      "turbo             int64\n",
      "is_automatic    float64\n",
      "gears           float64\n",
      "is_hybrid         int64\n",
      "is_gasoline       int64\n",
      "is_electric       int64\n",
      "is_luxury         int64\n",
      "engine_score    float64\n",
      "dtype: object\n",
      "   model_year    milage  accident  clean_title  price  turbo  is_automatic  \\\n",
      "0        2000  194277.0         1            1   2300      0           1.0   \n",
      "1        2015   13300.0         0            1  62500      0           1.0   \n",
      "2        2020   30426.0         1            0  29645      0           1.0   \n",
      "3        2020   67072.0         0            1  38500      0           1.0   \n",
      "4        2016   99000.0         1            1   5000      0           1.0   \n",
      "5        2020   16425.0         1            0  33200      0           1.0   \n",
      "6        2013   53900.0         1            0  27450      0           1.0   \n",
      "7        2014  146700.0         1            1  24999      0           1.0   \n",
      "8        2017   94000.0         0            1  26000      0           1.0   \n",
      "9        2022   46347.0         0            0  46120      0           1.0   \n",
      "\n",
      "   gears  is_hybrid  is_gasoline  is_electric  is_luxury  engine_score  \n",
      "0    NaN          0            1            0          0        -0.318  \n",
      "1    NaN          0            1            0          0         0.682  \n",
      "2    9.0          0            1            0          0        -0.454  \n",
      "3    NaN          0            1            0          0         0.136  \n",
      "4    6.0          0            1            0          0        -0.714  \n",
      "5    NaN          1            0            0          0        -0.454  \n",
      "6    7.0          0            1            0          0         0.504  \n",
      "7    5.0          0            1            0          0        -0.048  \n",
      "8    6.0          0            1            0          0        -0.109  \n",
      "9    8.0          0            1            0          0         0.467  \n",
      "model_year        int64\n",
      "milage          float64\n",
      "accident          int64\n",
      "price           float64\n",
      "turbo             int64\n",
      "is_automatic    float64\n",
      "gears           float64\n",
      "dual_shift        int64\n",
      "is_hybrid         int64\n",
      "is_luxury         int64\n",
      "engine_score    float64\n",
      "dtype: object\n",
      "   model_year    milage  accident    price  turbo  is_automatic  gears  \\\n",
      "0        2020   41500.0         0  51500.0      0           1.0    NaN   \n",
      "1        2004   99600.0         0   7500.0      0           0.0    5.0   \n",
      "2        2004  229450.0         0   9900.0      0           1.0    NaN   \n",
      "3        2019   24500.0         0  42500.0      0           NaN    NaN   \n",
      "4        2015   76500.0         0  21250.0      0           1.0    6.0   \n",
      "5        2023    1400.0         0  62900.0      0           1.0   10.0   \n",
      "6        2007  143250.0         0   9200.0      0           1.0    NaN   \n",
      "7        2005  103466.0         0  11000.0      0           1.0    5.0   \n",
      "8        2012   87500.0         1  11800.0      0           0.0    6.0   \n",
      "9        2013  173000.0         1  26750.0      0           1.0    6.0   \n",
      "\n",
      "   dual_shift  is_hybrid  is_luxury  engine_score  \n",
      "0           0          1          0        -0.048  \n",
      "1           0          0          0        -1.138  \n",
      "2           0          0          0         0.343  \n",
      "3           1          0          0         1.150  \n",
      "4           0          0          0         0.471  \n",
      "5           0          0          0        -0.145  \n",
      "6           0          0          0        -0.036  \n",
      "7           0          0          0        -0.143  \n",
      "8           0          0          0         0.740  \n",
      "9           0          0          0         1.176  \n",
      "model_year        int64\n",
      "milage          float64\n",
      "accident          int64\n",
      "clean_title       int64\n",
      "price             int64\n",
      "turbo             int64\n",
      "gears           float64\n",
      "is_electric       int64\n",
      "is_luxury         int64\n",
      "engine_score    float64\n",
      "dtype: object\n",
      "   model_year    milage  accident  clean_title  price  turbo  gears  \\\n",
      "0        2015   51300.0         0            1  17500      0    5.0   \n",
      "1        2004  148000.0         0            1  19000      0    4.0   \n",
      "2        2022    6600.0         1            1  72900      0   10.0   \n",
      "3        2012  110000.0         0            1  15600      0    NaN   \n",
      "4        2012    1295.0         0            1  49990      0    NaN   \n",
      "5        2014  163019.0         1            1   8900      0    6.0   \n",
      "6        2015  149800.0         1            1  11600      0    NaN   \n",
      "7        2017   75000.0         0            1  20000      0    NaN   \n",
      "8        2019   44500.0         0            1  28900      0    9.0   \n",
      "9        2020   24108.0         0            1  56000      0    9.0   \n",
      "\n",
      "   is_electric  is_luxury  engine_score  \n",
      "0            0          0        -0.793  \n",
      "1            0          0         0.253  \n",
      "2            0          0         1.026  \n",
      "3            0          0         0.692  \n",
      "4            0          0         0.691  \n",
      "5            0          0        -0.640  \n",
      "6            0          0        -0.599  \n",
      "7            0          0        -0.537  \n",
      "8            0          0        -0.561  \n",
      "9            0          0        -0.522  \n"
     ]
    }
   ],
   "source": [
    "train_dataset_original = pd.read_csv('data/1_Preprocessing/train.csv')\n",
    "print(train_dataset_original.dtypes)\n",
    "print(train_dataset_original.head(10))\n",
    "\n",
    "train_dataset_mixed = pd.read_csv('data/1_Preprocessing/train_generated_and_original.csv')\n",
    "print(train_dataset_mixed.dtypes)\n",
    "print(train_dataset_mixed.head(10))\n",
    "\n",
    "test_dataset = pd.read_csv('data/1_Preprocessing/test.csv')\n",
    "print(test_dataset.dtypes)\n",
    "print(test_dataset.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features and target variable\n",
    "X_train_original = train_dataset_original.drop(columns=['price'])\n",
    "y_train_original = train_dataset_original['price']\n",
    "\n",
    "X_train_mixed = train_dataset_mixed.drop(columns=['price'])\n",
    "y_train_mixed = train_dataset_mixed['price']\n",
    "\n",
    "X_test = test_dataset.drop(columns=['price'])\n",
    "y_test = test_dataset['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from itertools import product\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def hyperparameter_tuning_with_cv(X, y, param_grid, model_class=LinearRegression, n_splits=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning with K-Fold cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - X (DataFrame or ndarray): Feature matrix.\n",
    "    - y (Series or ndarray): Target vector.\n",
    "    - param_grid (dict): Hyperparameter grid as a dictionary of parameter lists.\n",
    "    - model_class (class): Model class to be instantiated (default: LinearRegression).\n",
    "    - n_splits (int): Number of splits for K-Fold cross-validation.\n",
    "    - random_state (int): Random state for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Best hyperparameters and performance metrics.\n",
    "    - list: All results for each hyperparameter combination.\n",
    "    \"\"\"\n",
    "    # Generate all combinations of hyperparameters\n",
    "    param_combinations = list(product(*param_grid.values()))\n",
    "    param_array = [dict(zip(param_grid.keys(), combo)) for combo in param_combinations]\n",
    "\n",
    "    # Initialize variables to store results\n",
    "    results = []\n",
    "\n",
    "    # RobustScaler for consistent scaling\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    # Iterate over all hyperparameter combinations\n",
    "    for params in param_array:\n",
    "        print(f\"Testing hyperparameters: {params}\")\n",
    "        rmse_scores = []  # Store RMSE for each fold\n",
    "        r2_scores = []   # Store R² for each fold\n",
    "\n",
    "        # Perform K-Fold cross-validation\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            # Split the data\n",
    "            X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            # Scale the data\n",
    "            X_train_fold = scaler.fit_transform(X_train_fold)\n",
    "            X_test_fold = scaler.transform(X_test_fold)\n",
    "\n",
    "            # Create a new model for this fold with the current hyperparameters\n",
    "            model = model_class(**params)\n",
    "\n",
    "            # Fit the model\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "            # Predict on the test fold\n",
    "            y_pred = model.predict(X_test_fold)\n",
    "\n",
    "            # Calculate metrics\n",
    "            mse = mean_squared_error(y_test_fold, y_pred)\n",
    "            r2 = r2_score(y_test_fold, y_pred)\n",
    "\n",
    "            # Store results for this fold\n",
    "            rmse_scores.append(np.sqrt(mse))\n",
    "            r2_scores.append(r2)\n",
    "\n",
    "        # Calculate mean R² and RMSE and store results\n",
    "        mean_r2 = np.mean(r2_scores)\n",
    "        mean_rmse = np.mean(rmse_scores)\n",
    "        results.append({'params': params, 'mean_r2': mean_r2, 'mean_rmse': mean_rmse})\n",
    "        print(f\"Mean R² for {params}: {mean_r2:.4f}\")\n",
    "\n",
    "    # Select the best hyperparameters\n",
    "    best_result = max(results, key=lambda x: x['mean_r2'])\n",
    "    print(f\"Best hyperparameters: {best_result['params']} with Mean R²: {best_result['mean_r2']:.4f}\")\n",
    "    return best_result, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualization\n",
    "def plot_results(y_true, y_pred):\n",
    "    # 1. Actual vs Predicted\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=y_true, y=y_pred, alpha=0.7, s=60)\n",
    "    plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], 'r--', linewidth=2)\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title('Actual vs Predicted with Regression Line')\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Residuals\n",
    "    residuals = y_true - y_pred\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=y_pred, y=residuals, alpha=0.7)\n",
    "    plt.axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residual Plot')\n",
    "    plt.show()\n",
    "\n",
    "    # 3. Residual Histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(residuals, kde=True, bins=30, color='blue', alpha=0.7)\n",
    "    plt.axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "    plt.xlabel('Residuals')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Residuals')\n",
    "    plt.show()\n",
    "\n",
    "    # 4. Q-Q Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "    plt.title('Q-Q Plot of Residuals')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'fit_intercept': [True, False]\n",
    "}\n",
    "\n",
    "best_result_original, all_results_original = hyperparameter_tuning_with_cv(X_train_original, y_train_original, param_grid)\n",
    "best_result_mixed, all_results_mixed = hyperparameter_tuning_with_cv(X_train_mixed, y_train_mixed, param_grid)\n",
    "\n",
    "#################\n",
    "# Original data #\n",
    "#################\n",
    "\n",
    "best_params_original = best_result_original['params']\n",
    "scaler = RobustScaler()\n",
    "\n",
    "X_train_original_copy = X_train_original.copy()\n",
    "y_train_original_copy = y_train_original.copy()\n",
    "X_test_copy = X_test.copy()\n",
    "y_test_copy = y_test.copy()\n",
    "\n",
    "X_train_scaled_original = scaler.fit_transform(X_train_original_copy)\n",
    "X_test_scaled_original = scaler.transform(X_test_copy)\n",
    "final_model = LinearRegression(**best_params_original)\n",
    "final_model.fit(X_train_scaled_original, y_train_original_copy)\n",
    "\n",
    "# Generate predictions for evaluation\n",
    "y_pred_full_original = final_model.predict(X_test_scaled_original)\n",
    "plot_results(y_test_copy, y_pred_full_original)\n",
    "\n",
    "#################\n",
    "#   Mixed data  #\n",
    "#################\n",
    "\n",
    "mixed = best_result_mixed['params']\n",
    "scaler = RobustScaler()\n",
    "\n",
    "X_train_mixed_copy = X_train_mixed.copy()\n",
    "y_train_mixed_copy = y_train_mixed.copy()\n",
    "X_test_copy = X_test.copy()\n",
    "y_test_copy = y_test.copy()\n",
    "\n",
    "X_train_scaled_mixed = scaler.fit_transform(X_train_mixed_copy)\n",
    "X_test_scaled_mixed = scaler.transform(X_test_copy)\n",
    "final_model = LinearRegression(**best_params_original)\n",
    "final_model.fit(X_train_scaled_mixed, y_train_mixed_copy)\n",
    "\n",
    "# Generate predictions for evaluation\n",
    "y_pred_full_mixed = final_model.predict(X_test_scaled_original)\n",
    "plot_results(y_test_copy, y_pred_full_mixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing hyperparameters: {'fit_intercept': True}\n",
      "Mean R² for {'fit_intercept': True}: 0.3057\n",
      "Testing hyperparameters: {'fit_intercept': False}\n",
      "Mean R² for {'fit_intercept': False}: -0.0841\n",
      "Best hyperparameters: {'fit_intercept': True} with Mean R²: 0.3057\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 91\u001b[0m\n\u001b[1;32m     88\u001b[0m X_train_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X_train_copy)\n\u001b[1;32m     89\u001b[0m X_test_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit(X_test_copy)\n\u001b[0;32m---> 91\u001b[0m \u001b[43mfinal_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Predict on the full dataset\u001b[39;00m\n\u001b[1;32m     94\u001b[0m y_pred_full \u001b[38;5;241m=\u001b[39m final_model\u001b[38;5;241m.\u001b[39mpredict(X_test_scaled)\n",
      "File \u001b[0;32m~/Documents/Obsidian Vault/Uni/UniMannheim/Studium/Semester 1/Data Mining/Projekt/RandomForestRangers/venv/lib/python3.13/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Obsidian Vault/Uni/UniMannheim/Studium/Semester 1/Data Mining/Projekt/RandomForestRangers/venv/lib/python3.13/site-packages/sklearn/linear_model/_base.py:609\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    605\u001b[0m n_jobs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs\n\u001b[1;32m    607\u001b[0m accept_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 609\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    618\u001b[0m has_sw \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_sw:\n",
      "File \u001b[0;32m~/Documents/Obsidian Vault/Uni/UniMannheim/Studium/Semester 1/Data Mining/Projekt/RandomForestRangers/venv/lib/python3.13/site-packages/sklearn/base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/Documents/Obsidian Vault/Uni/UniMannheim/Studium/Semester 1/Data Mining/Projekt/RandomForestRangers/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:1301\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1296\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1299\u001b[0m     )\n\u001b[0;32m-> 1301\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1318\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1320\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/Documents/Obsidian Vault/Uni/UniMannheim/Studium/Semester 1/Data Mining/Projekt/RandomForestRangers/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1061\u001b[0m     )\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Obsidian Vault/Uni/UniMannheim/Studium/Semester 1/Data Mining/Projekt/RandomForestRangers/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Obsidian Vault/Uni/UniMannheim/Studium/Semester 1/Data Mining/Projekt/RandomForestRangers/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m     )\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'fit_intercept': [True, False]\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "param_combinations = list(product(\n",
    "    param_grid['fit_intercept'],\n",
    "))\n",
    "\n",
    "param_array = [\n",
    "    {\n",
    "        'fit_intercept': combo[0],\n",
    "    } for combo in param_combinations\n",
    "]\n",
    "\n",
    "# Initialize variables to store results\n",
    "results = []\n",
    "\n",
    "# RobustScaler for consistent scaling\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Iterate over all hyperparameter combinations\n",
    "for params in param_array:\n",
    "    print(f\"Testing hyperparameters: {params}\")\n",
    "    rmse_scores = []  # Store MSE for each fold\n",
    "    r2_scores = []   # Store R² for each fold\n",
    "\n",
    "    # Perform K-Fold cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        # Split the data\n",
    "        X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Scale the data\n",
    "        X_train_fold = scaler.fit_transform(X_train_fold)\n",
    "        X_test_fold = scaler.transform(X_test_fold)\n",
    "\n",
    "        # Create a new model for this fold with the current hyperparameters\n",
    "        model = LinearRegression(**params)\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Predict on the test fold\n",
    "        y_pred = model.predict(X_test_fold)\n",
    "\n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_test_fold, y_pred)\n",
    "        r2 = r2_score(y_test_fold, y_pred)\n",
    "\n",
    "        # Store results for this fold\n",
    "        rmse_scores.append(np.sqrt(mse))\n",
    "        r2_scores.append(r2)\n",
    "\n",
    "    # Calculate mean R² and store results\n",
    "    mean_r2 = np.mean(r2_scores)\n",
    "    mean_rmse = np.mean(rmse_scores)\n",
    "    results.append({'params': params, 'mean_r2': mean_r2, 'mean_rmse': mean_rmse})\n",
    "    print(f\"Mean R² for {params}: {mean_r2:.4f}\")\n",
    "\n",
    "# Select the best hyperparameters\n",
    "best_result = max(results, key=lambda x: x['mean_r2'])\n",
    "best_params = best_result['params']\n",
    "print(f\"Best hyperparameters: {best_params} with Mean R²: {best_result['mean_r2']:.4f}\")\n",
    "\n",
    "# Train the final model with the best hyperparameters on the full dataset\n",
    "X_train_copy = X_train_original.copy()\n",
    "y_train_copy = y_train_original.copy()\n",
    "X_test_copy = X_test.copy()\n",
    "y_test_copy = y_test.copy()\n",
    "\n",
    "# Train final model on the full dataset with the best hyperparameters\n",
    "final_model = LinearRegression(**best_params)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train_copy)\n",
    "X_test_scaled = scaler.fit(X_test_copy)\n",
    "\n",
    "final_model.fit(X_train_scaled, y_train_copy)\n",
    "\n",
    "# Predict on the full dataset\n",
    "y_pred_full = final_model.predict(X_test_scaled)\n",
    "print(f\"R2 for best model: {r2_score(y_test_copy, y_pred_full)}\")\n",
    "\n",
    "# --- Visualization ---\n",
    "# 1. Plot: Actual vs Predicted with the Regression Line\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_test_copy, y=y_pred_full, label=\"Predicted vs Actual\", alpha=0.7, s=60)\n",
    "plt.plot([min(y_test_copy), max(y_test_copy)], [min(y_test_copy), max(y_test_copy)], 'r--', label='Perfect Fit Line', linewidth=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs Predicted with Regression Line')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 2. Residual Plot\n",
    "residuals = y - y_pred_full\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_pred_full, y=residuals, alpha=0.7)\n",
    "plt.hlines(y=0, xmin=min(y_pred_full), xmax=max(y_pred_full), colors='red', linestyles='--', linewidth=2)\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.show()\n",
    "\n",
    "# 3. Histogram of Residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, kde=True, color='blue', bins=30, alpha=0.7)\n",
    "plt.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "plt.title('Histogram of Residuals')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# 4. Q-Q Plot\n",
    "import scipy.stats as stats\n",
    "plt.figure(figsize=(10, 6))\n",
    "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'n_estimators': 10, 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1}, {'n_estimators': 10, 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 2}, {'n_estimators': 10, 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 4}, {'n_estimators': 10, 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 1}, {'n_estimators': 10, 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 2}, {'n_estimators': 10, 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 4}, {'n_estimators': 10, 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 1}, {'n_estimators': 10, 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 2}, {'n_estimators': 10, 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 4}, {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}, {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2}, {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 4}, {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1}, {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2}, {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 4}, {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1}, {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2}, {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 4}, {'n_estimators': 10, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 1}, {'n_estimators': 10, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 2}, {'n_estimators': 10, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 4}, {'n_estimators': 10, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 1}, {'n_estimators': 10, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 2}, {'n_estimators': 10, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 4}, {'n_estimators': 10, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 1}, {'n_estimators': 10, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 2}, {'n_estimators': 10, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 4}, {'n_estimators': 50, 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1}, {'n_estimators': 50, 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 2}, {'n_estimators': 50, 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 4}, {'n_estimators': 50, 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 1}, {'n_estimators': 50, 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 2}, {'n_estimators': 50, 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 4}, {'n_estimators': 50, 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 1}, {'n_estimators': 50, 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 2}, {'n_estimators': 50, 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 4}, {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}, {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2}, {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 4}, {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1}, {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2}, {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 4}, {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1}, {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2}, {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 4}, {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 1}, {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 2}, {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 4}, {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 1}, {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 2}, {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 4}, {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 1}, {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 2}, {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 4}, {'n_estimators': 100, 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1}, {'n_estimators': 100, 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 2}, {'n_estimators': 100, 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 4}, {'n_estimators': 100, 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 1}, {'n_estimators': 100, 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 2}, {'n_estimators': 100, 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 4}, {'n_estimators': 100, 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 1}, {'n_estimators': 100, 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 2}, {'n_estimators': 100, 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 4}, {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}, {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2}, {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 4}, {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1}, {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2}, {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 4}, {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1}, {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2}, {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 4}, {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 1}, {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 2}, {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 4}, {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 1}, {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 2}, {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 4}, {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 1}, {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 2}, {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 4}, {'n_estimators': 200, 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1}, {'n_estimators': 200, 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 2}, {'n_estimators': 200, 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 4}, {'n_estimators': 200, 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 1}, {'n_estimators': 200, 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 2}, {'n_estimators': 200, 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 4}, {'n_estimators': 200, 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 1}, {'n_estimators': 200, 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 2}, {'n_estimators': 200, 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 4}, {'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}, {'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2}, {'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 4}, {'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1}, {'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2}, {'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 4}, {'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1}, {'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2}, {'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 4}, {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 1}, {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 2}, {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 4}, {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 1}, {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 2}, {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 4}, {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 1}, {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 2}, {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 4}]\n",
      "Run 100 of <built-in function len>\n",
      "Best hyperparameters: {'n_estimators': 10, 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 4} with Mean R²: 0.5022\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from itertools import product\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "param_combinations = list(product(\n",
    "    param_grid['n_estimators'],\n",
    "    param_grid['max_depth'],\n",
    "    param_grid['min_samples_split'],\n",
    "    param_grid['min_samples_leaf']\n",
    "))\n",
    "\n",
    "param_array = [\n",
    "    {\n",
    "        'n_estimators': combo[0],\n",
    "        'max_depth': combo[1],\n",
    "        'min_samples_split': combo[2],\n",
    "        'min_samples_leaf': combo[3]\n",
    "    } for combo in param_combinations\n",
    "]\n",
    "\n",
    "# Initialize variables to store results\n",
    "results = []\n",
    "\n",
    "# RobustScaler for consistent scaling\n",
    "scaler = RobustScaler()\n",
    "index = 1\n",
    "length = len(param_array)\n",
    "\n",
    "# Iterate over all hyperparameter combinations\n",
    "for params in param_array:\n",
    "    mse_scores = []  # Store MSE for each fold\n",
    "    r2_scores = []   # Store R² for each fold\n",
    "\n",
    "    # Perform K-Fold cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        # Split the data\n",
    "        X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Scale the data\n",
    "        X_train_fold = scaler.fit_transform(X_train_fold)\n",
    "        X_test_fold = scaler.transform(X_test_fold)\n",
    "\n",
    "        # Create a new model for this fold with the current hyperparameters\n",
    "        model = RandomForestRegressor(**params)\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Predict on the test fold\n",
    "        y_pred = model.predict(X_test_fold)\n",
    "\n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_test_fold, y_pred)\n",
    "        r2 = r2_score(y_test_fold, y_pred)\n",
    "\n",
    "        # Store results for this fold\n",
    "        mse_scores.append(mse)\n",
    "        r2_scores.append(r2)\n",
    "\n",
    "    # Calculate mean R² and store results\n",
    "    mean_r2 = np.mean(r2_scores)\n",
    "    results.append({'params': params, 'mean_r2': mean_r2})\n",
    "    if index % 100 == 0:\n",
    "        print(f\"Run {index} of {len}\")\n",
    "    index = index + 1\n",
    "\n",
    "# Train the final model with the best hyperparameters on the full dataset\n",
    "X_copy = X.copy()\n",
    "y_copy = y.copy()\n",
    "\n",
    "# Select the best hyperparameters\n",
    "best_result = max(results, key=lambda x: x['mean_r2'])\n",
    "best_params = best_result['params']\n",
    "print(f\"Best hyperparameters: {best_params} with Mean R²: {best_result['mean_r2']:.4f}\")\n",
    "\n",
    "# Train final model on the full dataset with the best hyperparameters\n",
    "final_model = RandomForestRegressor(**best_params)\n",
    "X_scaled = scaler.fit_transform(X_copy)\n",
    "final_model.fit(X_scaled, y_copy)\n",
    "\n",
    "# Predict on the full dataset\n",
    "y_pred_full = final_model.predict(X_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1:\n",
      "MSE: 16839261939.352365\n",
      "RMSE: 129766.18180154783\n",
      "R2: 0.17614406386072512\n",
      "Adjusted R2: 0.16888085031793554\n",
      "\n",
      "Run 2:\n",
      "MSE: 1328310954.1040113\n",
      "RMSE: 36446.00052274613\n",
      "R2: 0.5631424572909156\n",
      "Adjusted R2: 0.5592910683753444\n",
      "\n",
      "Run 3:\n",
      "MSE: 37611272031.830284\n",
      "RMSE: 193936.2576514002\n",
      "R2: -16.710491056944214\n",
      "Adjusted R2: -16.86662888742105\n",
      "\n",
      "Run 4:\n",
      "MSE: 1181310078.978533\n",
      "RMSE: 34370.19172158533\n",
      "R2: 0.6248431049986214\n",
      "Adjusted R2: 0.6215356764532691\n",
      "\n",
      "Run 5:\n",
      "MSE: 599176123.4474512\n",
      "RMSE: 24478.07434108025\n",
      "R2: 0.723074107522736\n",
      "Adjusted R2: 0.7206296166685862\n",
      "\n",
      "Average MSE: 11511866225.54\n",
      "Average R²: -2.92\n",
      "Average Adjusted R²: -2.96\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Initialize the XGBoost regressor\n",
    "model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Cross-validation loop\n",
    "mse_scores = []          # Store MSE for each fold\n",
    "r2_scores = []           # Store R² for each fold\n",
    "adjusted_r2_scores = []  # Store Adjusted R² for each fold\n",
    "\n",
    "n = len(train_dataset_original)  # Total number of samples\n",
    "p = X.shape[1]    # Number of predictors\n",
    "i = 1\n",
    "\n",
    "x_copy = X.copy()\n",
    "y_copy = y.copy()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split the data\n",
    "    X_train_fold, X_test_fold = x_copy.iloc[train_index], x_copy.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y_copy.iloc[train_index], y_copy.iloc[test_index]\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Predict on the test fold\n",
    "    y_pred = model.predict(X_test_fold)\n",
    "\n",
    "    # Calculate MSE\n",
    "    mse = mean_squared_error(y_test_fold, y_pred)\n",
    "    mse_scores.append(mse)\n",
    "\n",
    "    # Calculate R²\n",
    "    r2 = r2_score(y_test_fold, y_pred)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "    # Calculate Adjusted R²\n",
    "    n_fold = len(y_test_fold)  # Number of samples in this fold\n",
    "    adjusted_r2 = 1 - ((1 - r2) * (n_fold - 1)) / (n_fold - p - 1)\n",
    "    adjusted_r2_scores.append(adjusted_r2)\n",
    "\n",
    "    print(f\"Run {i}:\\nMSE: {mse}\\nRMSE: {np.sqrt(mse)}\\nR2: {r2}\\nAdjusted R2: {adjusted_r2}\\n\")\n",
    "    i = i + 1\n",
    "\n",
    "# Average metrics across folds\n",
    "avg_mse = sum(mse_scores) / len(mse_scores)\n",
    "avg_r2 = sum(r2_scores) / len(r2_scores)\n",
    "avg_adjusted_r2 = sum(adjusted_r2_scores) / len(adjusted_r2_scores)\n",
    "\n",
    "print(f\"Average MSE: {avg_mse:.2f}\")\n",
    "print(f\"Average R²: {avg_r2:.2f}\")\n",
    "print(f\"Average Adjusted R²: {avg_adjusted_r2:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
